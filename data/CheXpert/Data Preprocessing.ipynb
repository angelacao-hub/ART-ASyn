{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21895f59",
   "metadata": {},
   "source": [
    "# CheXpert Dataset Preprocessing Guide\n",
    "\n",
    "This notebook provides a comprehensive guide for downloading and preprocessing the CheXpert large chest X-ray dataset for for ART-ASyn.\n",
    "\n",
    "## Dataset Information\n",
    "- **Sources**:\n",
    "    - [Stanford AIMI - Full Resolution CheXpert Train & Validation Dataset](https://stanfordaimi.azurewebsites.net/datasets/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2)  OR  [Kaggle - Small CheXpert Train & Validation Dataset](https://www.kaggle.com/datasets/ashery/chexpert)\n",
    "    - [Stanford AIMI - CheXpert Test Dataset](https://stanfordaimi.azurewebsites.net/datasets/23c56a0d-15de-405b-87c8-99c30138950c)\n",
    "- **Type**: Large-scale chest X-ray dataset for multi-label classification\n",
    "- **Format**: JPEG images with CSV annotations\n",
    "\n",
    "## Dataset Structure\n",
    "The CheXpert dataset is organized into three main splits:\n",
    "- **Training Set**: `train.csv` with 191,027 frontal images\n",
    "- **Validation Set**: `valid.csv` with 202 frontal images  \n",
    "- **Test Set**: `test.csv` with 518 frontal images\n",
    "\n",
    "### 13 Medical Conditions\n",
    "The dataset includes labels for these chest pathologies:\n",
    "1. **Enlarged Cardiomediastinum** - Heart and mediastinum enlargement\n",
    "2. **Cardiomegaly** - Abnormal enlargement of the heart\n",
    "3. **Lung Opacity** - Opacities or shadows in lung fields\n",
    "4. **Lung Lesion** - Lung lesions or masses\n",
    "5. **Edema** - Pulmonary edema\n",
    "6. **Consolidation** - Lung consolidation\n",
    "7. **Pneumonia** - Pneumonia detection\n",
    "8. **Atelectasis** - Lung collapse\n",
    "9. **Pneumothorax** - Collapsed lung\n",
    "10. **Pleural Effusion** - Fluid in pleural space\n",
    "11. **Pleural Other** - Other pleural abnormalities\n",
    "12. **Fracture** - Bone fractures\n",
    "13. **Support Devices** - Medical devices present\n",
    "\n",
    "### Label Encoding System\n",
    "- **1.0**: Positive (Abnormal finding present)\n",
    "- **0.0**: Negative (Abnormal finding absent)\n",
    "- **-1.0**: Uncertain (Uncertain/equivocal findings)\n",
    "- **NaN**: Missing (Finding not mentioned)\n",
    "\n",
    "This preprocessing script organizes the data into a binary classification format (healthy vs diseased) suitable for machine learning models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb66654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All required libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "print(\"âœ… All required libraries imported successfully\")\n",
    "\n",
    "def create_dir(dir_name):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "### Step 1: Download the Dataset\n",
    "\n",
    "Before running this notebook, you need to download the CheXpert dataset. You have **two options**:\n",
    "\n",
    "#### Option 1. Stanford AIMI (Complete Train & Validation Dataset)\n",
    "1. **Visit**: https://stanfordaimi.azurewebsites.net/datasets/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2\n",
    "2. **Download Size**: 471.17 GB (Complete dataset)\n",
    "3. **Registration Required**: Must register for access\n",
    "4. **Download Command** (after registration):\n",
    "   ```bash\n",
    "   azcopy copy <download_url> <download-path>/data --recursive=true\n",
    "   ```\n",
    "\n",
    "#### Option 2: Kaggle (Recommended)\n",
    "1. **Visit**: https://www.kaggle.com/datasets/ashery/chexpert\n",
    "2. **Download Size**: 10.7GB\n",
    "3. **Direct Download**: Requires Kaggle account, download directly from the page\n",
    "4. **Structure**: Complete CheXpert-v1.0-small dataset ready for use\n",
    "\n",
    "#### Must download: Stanford AIMI - Test Dataset\n",
    "1. **Visit**: https://stanfordaimi.azurewebsites.net/datasets/23c56a0d-15de-405b-87c8-99c30138950c\n",
    "2. **Download Size**: 5.73GB\n",
    "3. **Registration Required**: Must register for access\n",
    "4. **Download Command** (after registration):\n",
    "   ```bash\n",
    "   azcopy copy <download_url> <download-path>/data --recursive=true\n",
    "   ```\n",
    "5. **File Structure After Download**:\n",
    "   ```\n",
    "   chexlocalize/\n",
    "   â””â”€â”€ CheXpert/\n",
    "       â”œâ”€â”€ test/\n",
    "       â”‚   â””â”€â”€ patient*/study*/images\n",
    "       â””â”€â”€ test_labels.csv  # Rename to test.csv\n",
    "   ```\n",
    "6. **Important**:\n",
    "    - Rename `test_labels.csv` to `test.csv` and place it inside the Kaggle dataset's `CheXpert-v1.0-small` folder\n",
    "    - Copy `test` folder and place it inside the Kaggle dataset's `CheXpert-v1.0-small` folder\n",
    "\n",
    "### Step 2: Extract and Organize Dataset\n",
    "\n",
    "Extract the dataset to create the following folder structure:\n",
    "```\n",
    "CheXpert-v1.0-small/     # Main dataset folder\n",
    "â”œâ”€â”€ train.csv           # Training labels\n",
    "â”œâ”€â”€ valid.csv           # Validation labels\n",
    "â”œâ”€â”€ test.csv            # Test labels (from Stanford AIMI)\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â””â”€â”€ patient*/study*/images/*.jpg\n",
    "â”œâ”€â”€ valid/\n",
    "â”‚   â””â”€â”€ patient*/study*/images/*.jpg\n",
    "â””â”€â”€ test/\n",
    "    â””â”€â”€ patient*/study*/images/*.jpg\n",
    "```\n",
    "\n",
    "**Note**: The main notebook directory should be at the same level as the `CheXpert-v1.0-small` folder:\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ CheXpert/\n",
    "â”‚   â””â”€â”€ Data Preprocessing.ipynb\n",
    "â””â”€â”€ CheXpert-v1.0-small/  # Dataset folder\n",
    "```\n",
    "\n",
    "### Step 3: Configure Data Paths\n",
    "\n",
    "Set the root directory where your CheXpert dataset is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294b6d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset directory found: ../CheXpert-v1.0-small\n",
      "ğŸ“ Directory contents: [WindowsPath('../CheXpert-v1.0-small/test'), WindowsPath('../CheXpert-v1.0-small/test.csv'), WindowsPath('../CheXpert-v1.0-small/train'), WindowsPath('../CheXpert-v1.0-small/train.csv'), WindowsPath('../CheXpert-v1.0-small/valid'), WindowsPath('../CheXpert-v1.0-small/valid.csv')]\n",
      "âœ… Found CSV: train.csv\n",
      "âœ… Found CSV: valid.csv\n",
      "âœ… Found CSV: test.csv\n"
     ]
    }
   ],
   "source": [
    "# Configure dataset root directory\n",
    "# Change this path to match your local dataset location\n",
    "root = \"../CheXpert-v1.0-small\"\n",
    "\n",
    "# Define CSV file paths\n",
    "train_csv = f\"{root}/train.csv\"\n",
    "val_csv = f\"{root}/valid.csv\"\n",
    "test_csv = f\"{root}/test.csv\"\n",
    "\n",
    "# Verify the dataset directory exists\n",
    "if Path(root).exists():\n",
    "    print(f\"âœ… Dataset directory found: {root}\")\n",
    "    print(f\"ğŸ“ Directory contents: {list(Path(root).iterdir())}\")\n",
    "    \n",
    "    # Check if required CSV files exist\n",
    "    csv_files = [train_csv, val_csv, test_csv]\n",
    "    for csv_file in csv_files:\n",
    "        if Path(csv_file).exists():\n",
    "            print(f\"âœ… Found CSV: {Path(csv_file).name}\")\n",
    "        else:\n",
    "            print(f\"âŒ Missing CSV: {Path(csv_file).name}\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset directory not found: {root}\")\n",
    "    print(\"Please ensure you have downloaded and extracted the dataset correctly.\")\n",
    "    print(\"Expected structure: ../CheXpert-v1.0-small/{train.csv,valid.csv,test.csv,train/,valid/,test/}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-section",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Overview\n",
    "\n",
    "The preprocessing pipeline will:\n",
    "1. **Load CSV annotations** for train, validation, and test sets\n",
    "2. **Filter frontal images only** (exclude lateral views)\n",
    "3. **Classify images** as healthy or diseased based on pathology labels\n",
    "4. **Handle uncertainty labels** (-1.0 values) appropriately\n",
    "5. **Organize images** by class and dataset split\n",
    "6. **Create structured directories** for ML workflows\n",
    "\n",
    "### Classification Logic\n",
    "\n",
    "Images are classified as:\n",
    "- **Healthy**: No positive pathology findings (all labels are 0.0, -1.0, or NaN)\n",
    "- **Diseased**: At least one positive pathology finding (label = 1.0)\n",
    "\n",
    "**Uncertainty Handling**: Images with only -1.0 (uncertain) and 0.0 (negative) labels are excluded to ensure clean binary classification.\n",
    "\n",
    "### Dataset Structure After Preprocessing\n",
    "\n",
    "```\n",
    "ğŸ“‚ CheXpert/\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ healthy/     # Images with no pathology findings\n",
    "â”‚   â””â”€â”€ diseased/   # Images with at least one pathology\n",
    "â”œâ”€â”€ val/\n",
    "â”‚   â”œâ”€â”€ healthy/     # Validation healthy images\n",
    "â”‚   â””â”€â”€ diseased/   # Validation diseased images\n",
    "â””â”€â”€ test/\n",
    "    â”œâ”€â”€ healthy/     # Test healthy images (resized to 25%)\n",
    "    â””â”€â”€ diseased/   # Test diseased images (resized to 25%)\n",
    "```\n",
    "\n",
    "### File Naming Convention\n",
    "\n",
    "CheXpert images follow the pattern: `{patient_id}/{study_id}/{image_name}.jpg`\n",
    "- `patient_id`: Unique patient identifier\n",
    "- `study_id`: Study session identifier  \n",
    "- `image_name`: Specific image file name\n",
    "\n",
    "**Example**: `patient00001/study01/view1_frontal.jpg`\n",
    "\n",
    "Images are renamed during preprocessing to: `{patient_id}_{study_id}.{extension}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-processing",
   "metadata": {},
   "source": [
    "## Training Data Preprocessing\n",
    "\n",
    "The following code processes the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbe65fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading training data...\n",
      "==================================================\n",
      "ğŸ“Š Total frontal training images: 191,027\n",
      "\n",
      "ğŸ” Analyzing pathology labels...\n",
      "   Healthy images (no pathology): 12,234\n",
      "   Diseased images (has pathology): 173,708\n",
      "   Uncertain images (excluded): 5,085\n",
      "\n",
      "âœ… Training data analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze training data\n",
    "print(\"ğŸ”„ Loading training data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "\n",
    "# Only select frontal images\n",
    "frontal = train_df[\"Path\"].str.endswith(\"_frontal.jpg\")\n",
    "train_df = train_df[frontal]\n",
    "\n",
    "print(f\"ğŸ“Š Total frontal training images: {len(train_df):,}\")\n",
    "\n",
    "train_paths = train_df[\"Path\"]\n",
    "\n",
    "# Analyze pathology labels\n",
    "print(\"\\nğŸ” Analyzing pathology labels...\")\n",
    "data = train_df.iloc[:, 6:].fillna(0)  # All columns of findings\n",
    "\n",
    "# Classification logic\n",
    "unsure_findings = (data.max(axis=1) == 0) & ((data.min(axis=1) == -1))  # Only uncertain findings\n",
    "unhealthy_idxs = ~unsure_findings & data.max(axis=1) == 1  # Has positive findings\n",
    "healthy_idxs = ~unsure_findings & ~unhealthy_idxs  # No positive findings\n",
    "\n",
    "print(f\"   Healthy images (no pathology): {sum(healthy_idxs):,}\")\n",
    "print(f\"   Diseased images (has pathology): {sum(unhealthy_idxs):,}\")\n",
    "print(f\"   Uncertain images (excluded): {sum(unsure_findings):,}\")\n",
    "\n",
    "print(\"\\nâœ… Training data analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc3e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process training data - Diseased images (Optional)\n",
    "# print(\"\\nğŸ”„ Processing training data - Diseased images...\")\n",
    "# print(\"=\" * 55)\n",
    "\n",
    "# target_dir = \"train/diseased\"\n",
    "# create_dir(target_dir)\n",
    "\n",
    "# diseased_count = 0\n",
    "# for path in tqdm(train_paths[unhealthy_idxs], desc=\"Processing diseased training images\"):\n",
    "#     source = f\"../{path}\"\n",
    "#     _, _, patient_id, study_id, img_name = path.split(\"/\")\n",
    "#     suffix = img_name.split(\".\")[-1]\n",
    "#     destination = f\"{target_dir}/{patient_id}_{study_id}.{suffix}\"\n",
    "    \n",
    "#     if not os.path.exists(destination):\n",
    "#         shutil.copy(source, destination)\n",
    "#         diseased_count += 1\n",
    "\n",
    "# print(f\"âœ… Processed {diseased_count:,} diseased training images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce15104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing training data - Healthy images...\n",
      "======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing healthy training images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12234/12234 [00:22<00:00, 542.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 11,924 healthy training images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process training data - Healthy images\n",
    "print(\"\\nğŸ”„ Processing training data - Healthy images...\")\n",
    "print(\"=\" * 54)\n",
    "\n",
    "target_dir = \"train/healthy\"\n",
    "create_dir(target_dir)\n",
    "\n",
    "healthy_count = 0\n",
    "for path in tqdm(train_paths[healthy_idxs], desc=\"Processing healthy training images\"):\n",
    "    source = f\"../{path}\"\n",
    "    _, _, patient_id, study_id, img_name = path.split(\"/\")\n",
    "    suffix = img_name.split(\".\")[-1]\n",
    "    destination = f\"{target_dir}/{patient_id}_{study_id}.{suffix}\"\n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        shutil.copy(source, destination)\n",
    "        healthy_count += 1\n",
    "\n",
    "print(f\"âœ… Processed {healthy_count:,} healthy training images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-processing",
   "metadata": {},
   "source": [
    "## Validation Data Preprocessing\n",
    "\n",
    "The following code processes the validation dataset using the same methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba88de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading validation data...\n",
      "==================================================\n",
      "ğŸ“Š Total frontal validation images: 202\n",
      "\n",
      "ğŸ” Analyzing validation pathology labels...\n",
      "   Healthy images (no pathology): 31\n",
      "   Diseased images (has pathology): 171\n",
      "   Uncertain images (excluded): 0\n",
      "\n",
      "âœ… Validation data analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze validation data\n",
    "print(\"ğŸ”„ Loading validation data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "val_df = pd.read_csv(val_csv)\n",
    "\n",
    "# Only select frontal images\n",
    "frontal = val_df[\"Path\"].str.endswith(\"_frontal.jpg\")\n",
    "val_df = val_df[frontal]\n",
    "\n",
    "print(f\"ğŸ“Š Total frontal validation images: {len(val_df):,}\")\n",
    "\n",
    "val_paths = val_df[\"Path\"]\n",
    "\n",
    "# Analyze pathology labels\n",
    "print(\"\\nğŸ” Analyzing validation pathology labels...\")\n",
    "data = val_df.iloc[:, 6:].fillna(0)  # All columns of findings\n",
    "\n",
    "# Classification logic (same as training)\n",
    "unsure_findings = (data.max(axis=1) == 0) & ((data.min(axis=1) == -1))\n",
    "unhealthy_idxs = ~unsure_findings & data.max(axis=1) == 1\n",
    "healthy_idxs = ~unsure_findings & ~unhealthy_idxs\n",
    "\n",
    "print(f\"   Healthy images (no pathology): {sum(healthy_idxs):,}\")\n",
    "print(f\"   Diseased images (has pathology): {sum(unhealthy_idxs):,}\")\n",
    "print(f\"   Uncertain images (excluded): {sum(unsure_findings):,}\")\n",
    "\n",
    "print(\"\\nâœ… Validation data analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce686e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing validation data - Diseased images...\n",
      "=========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing diseased validation images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 171/171 [00:00<00:00, 693.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 170 diseased validation images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process validation data - Diseased images\n",
    "print(\"\\nğŸ”„ Processing validation data - Diseased images...\")\n",
    "print(\"=\" * 57)\n",
    "\n",
    "target_dir = \"val/diseased\"\n",
    "create_dir(target_dir)\n",
    "\n",
    "diseased_count = 0\n",
    "for path in tqdm(val_paths[unhealthy_idxs], desc=\"Processing diseased validation images\"):\n",
    "    source = f\"../{path}\"\n",
    "    _, _, patient_id, study_id, img_name = path.split(\"/\")\n",
    "    suffix = img_name.split(\".\")[-1]\n",
    "    destination = f\"{target_dir}/{patient_id}_{study_id}.{suffix}\"\n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        shutil.copy(source, destination)\n",
    "        diseased_count += 1\n",
    "\n",
    "print(f\"âœ… Processed {diseased_count:,} diseased validation images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f513cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing validation data - Healthy images...\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing healthy validation images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 731.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 30 healthy validation images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process validation data - Healthy images\n",
    "print(\"\\nğŸ”„ Processing validation data - Healthy images...\")\n",
    "print(\"=\" * 56)\n",
    "\n",
    "target_dir = \"val/healthy\"\n",
    "create_dir(target_dir)\n",
    "\n",
    "healthy_count = 0\n",
    "for path in tqdm(val_paths[healthy_idxs], desc=\"Processing healthy validation images\"):\n",
    "    source = f\"../{path}\"\n",
    "    _, _, patient_id, study_id, img_name = path.split(\"/\")\n",
    "    suffix = img_name.split(\".\")[-1]\n",
    "    destination = f\"{target_dir}/{patient_id}_{study_id}.{suffix}\"\n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        shutil.copy(source, destination)\n",
    "        healthy_count += 1\n",
    "\n",
    "print(f\"âœ… Processed {healthy_count:,} healthy validation images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-processing",
   "metadata": {},
   "source": [
    "## Testing Data Preprocessing\n",
    "\n",
    "The following code processes the testing dataset. **Note**: Test images are resized to 25% of original size to reduce computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997887c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image resizing function\n",
    "def resize(image, ratio=0.25):\n",
    "    \"\"\"Resize image to specified ratio\"\"\"\n",
    "    w, h = image.size\n",
    "    width = int(w * ratio)\n",
    "    height = int(h * ratio)\n",
    "    return image.resize((width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27953cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading test data...\n",
      "==================================================\n",
      "ğŸ“Š Total frontal test images: 518\n",
      "\n",
      "ğŸ” Analyzing test pathology labels...\n",
      "   Healthy images (no pathology): 157\n",
      "   Diseased images (has pathology): 361\n",
      "   Uncertain images (excluded): 0\n",
      "\n",
      "âœ… Test data analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze test data\n",
    "print(\"ğŸ”„ Loading test data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "# Only select frontal images\n",
    "frontal = test_df[\"Path\"].str.endswith(\"_frontal.jpg\")\n",
    "test_df = test_df[frontal]\n",
    "\n",
    "print(f\"ğŸ“Š Total frontal test images: {len(test_df):,}\")\n",
    "\n",
    "test_paths = test_df[\"Path\"]\n",
    "\n",
    "# Analyze pathology labels\n",
    "print(\"\\nğŸ” Analyzing test pathology labels...\")\n",
    "data = test_df.iloc[:, 2:-1].fillna(0)  # All columns of findings\n",
    "\n",
    "# Classification logic (same as training/validation)\n",
    "unsure_findings = (data.max(axis=1) == 0) & ((data.min(axis=1) == -1))\n",
    "unhealthy_idxs = ~unsure_findings & data.max(axis=1) == 1\n",
    "healthy_idxs = ~unsure_findings & ~unhealthy_idxs\n",
    "\n",
    "print(f\"   Healthy images (no pathology): {sum(healthy_idxs):,}\")\n",
    "print(f\"   Diseased images (has pathology): {sum(unhealthy_idxs):,}\")\n",
    "print(f\"   Uncertain images (excluded): {sum(unsure_findings):,}\")\n",
    "\n",
    "print(\"\\nâœ… Test data analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1985ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing test data - Diseased images (resizing to 25%)...\n",
      "===================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing diseased test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 361/361 [00:17<00:00, 21.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 352 diseased test images (resized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process test data - Diseased images (with resizing)\n",
    "print(\"\\nğŸ”„ Processing test data - Diseased images (resizing to 25%)...\")\n",
    "print(\"=\" * 67)\n",
    "\n",
    "target_dir = \"test/diseased\"\n",
    "create_dir(target_dir)\n",
    "\n",
    "diseased_count = 0\n",
    "for path in tqdm(test_paths[unhealthy_idxs], desc=\"Processing diseased test images\"):\n",
    "    source = f\"../CheXpert-v1.0-small/{path}\"\n",
    "    _, patient_id, study_id, img_name = path.split(\"/\")\n",
    "    suffix = img_name.split(\".\")[-1]\n",
    "    destination = f\"{target_dir}/{patient_id}_{study_id}.{suffix}\"\n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        image = Image.open(source)\n",
    "        image = resize(image, ratio=0.25)\n",
    "        image.save(destination)\n",
    "        diseased_count += 1\n",
    "\n",
    "print(f\"âœ… Processed {diseased_count:,} diseased test images (resized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e91c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing test data - Healthy images (resizing to 25%)...\n",
      "==================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing healthy test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:06<00:00, 25.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 148 healthy test images (resized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process test data - Healthy images (with resizing)\n",
    "print(\"\\nğŸ”„ Processing test data - Healthy images (resizing to 25%)...\")\n",
    "print(\"=\" * 66)\n",
    "\n",
    "target_dir = \"test/healthy\"\n",
    "create_dir(target_dir)\n",
    "\n",
    "healthy_count = 0\n",
    "for path in tqdm(test_paths[healthy_idxs], desc=\"Processing healthy test images\"):\n",
    "    source = f\"../CheXpert-v1.0-small/{path}\"\n",
    "    _, patient_id, study_id, img_name = path.split(\"/\")\n",
    "    suffix = img_name.split(\".\")[-1]\n",
    "    destination = f\"{target_dir}/{patient_id}_{study_id}.{suffix}\"\n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        image = Image.open(source)\n",
    "        image = resize(image, ratio=0.25)\n",
    "        image.save(destination)\n",
    "        healthy_count += 1\n",
    "\n",
    "print(f\"âœ… Processed {healthy_count:,} healthy test images (resized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-summary",
   "metadata": {},
   "source": [
    "## Final Dataset Summary\n",
    "\n",
    "Let's verify the final dataset structure and get a comprehensive summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "final-summary-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Final Dataset Summary\n",
      "============================================================\n",
      "ğŸ“ Dataset Structure:\n",
      "   Training - Healthy    :  11924 images\n",
      "   Training - Diseased   : 106584 images\n",
      "   Validation - Healthy  :     30 images\n",
      "   Validation - Diseased :    170 images\n",
      "   Testing - Healthy     :    148 images\n",
      "   Testing - Diseased    :    352 images\n",
      "------------------------------------------------------------\n",
      "Total Training        : 118508 images\n",
      "Total Validation      :    200 images\n",
      "Total Testing         :    500 images\n",
      "Total Dataset         : 119208 images\n",
      "\n",
      "ğŸ“Š Class Distribution:\n",
      "   Training Set:\n",
      "      Healthy  :  11924 (10.1%)\n",
      "      Diseased : 106584 (89.9%)\n",
      "   Validation Set:\n",
      "      Healthy  :     30 (15.0%)\n",
      "      Diseased :    170 (85.0%)\n",
      "   Testing Set:\n",
      "      Healthy  :    148 (29.6%)\n",
      "      Diseased :    352 (70.4%)\n",
      "\n",
      "âœ… CheXpert dataset preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Final Dataset Verification and Summary\n",
    "print(\"ğŸ“‹ Final Dataset Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def count_files(directory):\n",
    "    \"\"\"Count files in a directory\"\"\"\n",
    "    return len(list(Path(directory).glob(\"*\"))) if Path(directory).exists() else 0\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_paths = {\n",
    "    \"Training - Healthy\": \"train/healthy\",\n",
    "    \"Training - Diseased\": \"train/diseased\", \n",
    "    \"Validation - Healthy\": \"val/healthy\",\n",
    "    \"Validation - Diseased\": \"val/diseased\",\n",
    "    \"Testing - Healthy\": \"test/healthy\",\n",
    "    \"Testing - Diseased\": \"test/diseased\"\n",
    "}\n",
    "\n",
    "# Count files and display summary\n",
    "total_train = 0\n",
    "total_val = 0\n",
    "total_test = 0\n",
    "\n",
    "print(\"ğŸ“ Dataset Structure:\")\n",
    "for label, path in dataset_paths.items():\n",
    "    count = count_files(path)\n",
    "    print(f\"   {label:<22}: {count:>6} images\")\n",
    "    \n",
    "    if \"Training\" in label:\n",
    "        total_train += count\n",
    "    elif \"Validation\" in label:\n",
    "        total_val += count\n",
    "    else:\n",
    "        total_test += count\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total Training':<22}: {total_train:>6} images\")\n",
    "print(f\"{'Total Validation':<22}: {total_val:>6} images\")\n",
    "print(f\"{'Total Testing':<22}: {total_test:>6} images\")\n",
    "print(f\"{'Total Dataset':<22}: {total_train + total_val + total_test:>6} images\")\n",
    "\n",
    "# Calculate class distribution\n",
    "train_healthy = count_files(\"train/healthy\")\n",
    "train_diseased = count_files(\"train/diseased\")\n",
    "val_healthy = count_files(\"val/healthy\")\n",
    "val_diseased = count_files(\"val/diseased\")\n",
    "test_healthy = count_files(\"test/healthy\")\n",
    "test_diseased = count_files(\"test/diseased\")\n",
    "\n",
    "print(\"\\nğŸ“Š Class Distribution:\")\n",
    "print(\"   Training Set:\")\n",
    "if total_train > 0:\n",
    "    print(f\"      Healthy  : {train_healthy:>6} ({train_healthy/total_train*100:.1f}%)\")\n",
    "    print(f\"      Diseased : {train_diseased:>6} ({train_diseased/total_train*100:.1f}%)\")\n",
    "print(\"   Validation Set:\")\n",
    "if total_val > 0:\n",
    "    print(f\"      Healthy  : {val_healthy:>6} ({val_healthy/total_val*100:.1f}%)\")\n",
    "    print(f\"      Diseased : {val_diseased:>6} ({val_diseased/total_val*100:.1f}%)\")\n",
    "print(\"   Testing Set:\")\n",
    "if total_test > 0:\n",
    "    print(f\"      Healthy  : {test_healthy:>6} ({test_healthy/total_test*100:.1f}%)\")\n",
    "    print(f\"      Diseased : {test_diseased:>6} ({test_diseased/total_test*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… CheXpert dataset preprocessing completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
